{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: [1, 8, 1, 16, 1, 7, 14, 6, 5, 2, 1, 1, 1, 41]\n",
      "input_dim: 105\n",
      "\n",
      "output_dim: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init global infos\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "inputs = (\n",
    "    (\"age\", (\"continuous\",)), \n",
    "    (\"workclass\", (\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\")), \n",
    "    (\"fnlwgt\", (\"continuous\",)), \n",
    "    (\"education\", (\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\")), \n",
    "    (\"education-num\", (\"continuous\",)), \n",
    "    (\"marital-status\", (\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\", \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\")), \n",
    "    (\"occupation\", (\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\", \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\", \"Armed-Forces\")), \n",
    "    (\"relationship\", (\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\", \"Unmarried\")), \n",
    "    (\"race\", (\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\")), \n",
    "    (\"sex\", (\"Female\", \"Male\")), \n",
    "    (\"capital-gain\", (\"continuous\",)), \n",
    "    (\"capital-loss\", (\"continuous\",)), \n",
    "    (\"hours-per-week\", (\"continuous\",)), \n",
    "    (\"native-country\", (\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\", \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\", \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"))\n",
    ")\n",
    "\n",
    "input_shape = []\n",
    "for i in inputs:\n",
    "    count = len(i[1 ])\n",
    "    input_shape.append(count)\n",
    "input_dim = sum(input_shape)\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"input_dim:\", input_dim)\n",
    "print()\n",
    "\n",
    "\n",
    "outputs = (0, 1)  # (\">50K\", \"<=50K\")\n",
    "output_dim = 2  # len(outputs)\n",
    "print(\"output_dim:\", output_dim)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load and prepare data\n",
    "\n",
    "def isFloat(string):\n",
    "    # credits: http://stackoverflow.com/questions/2356925/how-to-check-whether-string-might-be-type-cast-to-float-in-python\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def find_means_for_continuous_types(X):\n",
    "    means = []\n",
    "    for col in range(len(X[0])):\n",
    "        summ = 0\n",
    "        count = 0.000000000000000000001\n",
    "        for value in X[:, col]:\n",
    "            if isFloat(value): \n",
    "                summ += float(value)\n",
    "                count +=1\n",
    "        means.append(summ/count)\n",
    "    return means\n",
    "\n",
    "def prepare_data(raw_data, means):\n",
    "    \n",
    "    X = raw_data[:, :-1]\n",
    "    y = raw_data[:, -1:]\n",
    "    \n",
    "    # X:\n",
    "    def flatten_persons_inputs_for_model(person_inputs):\n",
    "        global inputs\n",
    "        global input_shape\n",
    "        global input_dim\n",
    "        global means\n",
    "        float_inputs = []\n",
    "\n",
    "        for i in range(len(input_shape)):\n",
    "            features_of_this_type = input_shape[i]\n",
    "            is_feature_continuous = features_of_this_type == 1\n",
    "\n",
    "            if is_feature_continuous:\n",
    "                mean = means[i]\n",
    "                if isFloat(person_inputs[i]):\n",
    "                    scale_factor = 1/(2*mean)  # we prefer inputs mainly scaled from -1 to 1. \n",
    "                    float_inputs.append(float(person_inputs[i])*scale_factor)\n",
    "                else:\n",
    "                    float_inputs.append(mean)\n",
    "            else:\n",
    "                for j in range(features_of_this_type):\n",
    "                    feature_name = inputs[i][1][j]\n",
    "\n",
    "                    if feature_name == person_inputs[i]:\n",
    "                        float_inputs.append(1.)\n",
    "                    else:\n",
    "                        float_inputs.append(0)\n",
    "        return float_inputs\n",
    "    \n",
    "    new_X = []\n",
    "    for person in range(len(X)):\n",
    "        formatted_X = flatten_persons_inputs_for_model(X[person])\n",
    "        new_X.append(formatted_X)\n",
    "    new_X = np.array(new_X)\n",
    "    \n",
    "    # y:\n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == \">50k\":\n",
    "            new_y.append((1, 0))\n",
    "        else:  # y[i] == \"<=50k\":\n",
    "            new_y.append((0, 1))\n",
    "    new_y = np.array(new_y)\n",
    "    \n",
    "    return (new_X, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 32561\n",
      "Test data count: 16281\n",
      "Mean values for data types (if continuous): [38.64358543876172, 0.0, 189664.13459727284, 0.0, 10.078088530363212, 0.0, 0.0, 0.0, 0.0, 0.0, 1079.0676262233324, 87.50231358257237, 40.422382375824085, 0.0, 0.0]\n",
      "Training data percentage that is >50k: 24.080955744602438 %\n"
     ]
    }
   ],
   "source": [
    "# Building training and test data\n",
    "\n",
    "training_data = np.genfromtxt('/Users/stetsyk/Desktop/data/adult.data.txt', delimiter=', ', dtype=str, autostrip=True)\n",
    "print(\"Training data count:\", len(training_data))\n",
    "test_data = np.genfromtxt('/Users/stetsyk/Desktop/data/adult.test.txt', delimiter=', ', dtype=str, autostrip=True)\n",
    "print(\"Test data count:\", len(test_data))\n",
    "\n",
    "means = find_means_for_continuous_types(np.concatenate((training_data, test_data), 0))\n",
    "print(\"Mean values for data types (if continuous):\", means)\n",
    "\n",
    "X_train, y_train = prepare_data(training_data, means)\n",
    "X_test, y_test = prepare_data(test_data, means)\n",
    "\n",
    "percent = sum([i[0] for i in y_train])/len(y_train)\n",
    "print(\"Training data percentage that is >50k:\", percent*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data format example:\n",
      "[0.36228522 1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.89212702 1.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.64496357 1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         1.\n",
      " 0.         0.         0.         0.49477539 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "\n",
      "In fact, we just crushed the data in such a way that it will optimise the neural network (model). \n",
      "It is crushed according to the `input_shape` variable: \n",
      "    say, if there are 41 native countries in the dataset, there will be 41 input dimensions for the \n",
      "    neural network with a value of 0 for every 41 input node for a given person, except the \n",
      "    node representing the real country of the person which will have a value of 1. For continuous \n",
      "    values, they are normalised to a small float number.\n"
     ]
    }
   ],
   "source": [
    "# Explanation on data format\n",
    "\n",
    "print(\"Training data format example:\")\n",
    "print(X_train[4])  # 4 is a random person, from cuba. \n",
    "print()\n",
    "\n",
    "print(\"In fact, we just crushed the data in such a way that it will optimise the neural network (model). \\n\\\n",
    "It is crushed according to the `input_shape` variable: \\n\\\n",
    "    say, if there are 41 native countries in the dataset, there will be 41 input dimensions for the \\n\\\n",
    "    neural network with a value of 0 for every 41 input node for a given person, except the \\n\\\n",
    "    node representing the real country of the person which will have a value of 1. For continuous \\n\\\n",
    "    values, they are normalised to a small float number.\")\n",
    "\n",
    "for i in X_train:\n",
    "    if len(i) != input_dim:\n",
    "        raise Exception(\n",
    "            \"Every person should have 105 data fields now. {} here.\".format(len(i)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_dim = 12\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(mid_dim, activation='tanh', input_dim=input_dim))\n",
    "model.add(Dense(100, activation='tanh'))\n",
    "model.add(Dense(100, activation='tanh'))\n",
    "model.add(Dense(100, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(output_dim, activation='tanh', input_dim=mid_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(training_datas, dimension): (32561, 105)\n"
     ]
    }
   ],
   "source": [
    "print(\"(training_datas, dimension):\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29304 samples, validate on 3257 samples\n",
      "Epoch 1/20\n",
      "29304/29304 [==============================] - 1s 43us/step - loss: 0.1161 - val_loss: 0.1090\n",
      "Epoch 2/20\n",
      "29304/29304 [==============================] - 1s 21us/step - loss: 0.1064 - val_loss: 0.1045\n",
      "Epoch 3/20\n",
      "29304/29304 [==============================] - 1s 21us/step - loss: 0.1046 - val_loss: 0.1038\n",
      "Epoch 4/20\n",
      "29304/29304 [==============================] - 1s 25us/step - loss: 0.1035 - val_loss: 0.1025\n",
      "Epoch 5/20\n",
      "29304/29304 [==============================] - 1s 23us/step - loss: 0.1033 - val_loss: 0.1030\n",
      "Epoch 6/20\n",
      "29304/29304 [==============================] - 1s 22us/step - loss: 0.1026 - val_loss: 0.1062\n",
      "Epoch 7/20\n",
      "29304/29304 [==============================] - 1s 21us/step - loss: 0.1021 - val_loss: 0.1033\n",
      "Epoch 8/20\n",
      "29304/29304 [==============================] - 1s 24us/step - loss: 0.1018 - val_loss: 0.1026\n",
      "Epoch 9/20\n",
      "29304/29304 [==============================] - 1s 21us/step - loss: 0.1017 - val_loss: 0.1034\n",
      "Epoch 10/20\n",
      "29304/29304 [==============================] - 1s 22us/step - loss: 0.1011 - val_loss: 0.1014\n",
      "Epoch 11/20\n",
      "29304/29304 [==============================] - 1s 24us/step - loss: 0.1011 - val_loss: 0.1001\n",
      "Epoch 12/20\n",
      "29304/29304 [==============================] - 1s 24us/step - loss: 0.1005 - val_loss: 0.1008\n",
      "Epoch 13/20\n",
      "29304/29304 [==============================] - 1s 28us/step - loss: 0.1006 - val_loss: 0.1001\n",
      "Epoch 14/20\n",
      "29304/29304 [==============================] - 1s 30us/step - loss: 0.1002 - val_loss: 0.1016\n",
      "Epoch 15/20\n",
      "29304/29304 [==============================] - 1s 27us/step - loss: 0.1003 - val_loss: 0.1023\n",
      "Epoch 16/20\n",
      "29304/29304 [==============================] - 1s 27us/step - loss: 0.0998 - val_loss: 0.1002\n",
      "Epoch 17/20\n",
      "29304/29304 [==============================] - 1s 30us/step - loss: 0.1002 - val_loss: 0.1023\n",
      "Epoch 18/20\n",
      "29304/29304 [==============================] - 1s 22us/step - loss: 0.0998 - val_loss: 0.1016\n",
      "Epoch 19/20\n",
      "29304/29304 [==============================] - 1s 22us/step - loss: 0.0995 - val_loss: 0.1009\n",
      "Epoch 20/20\n",
      "29304/29304 [==============================] - 1s 23us/step - loss: 0.0995 - val_loss: 0.1005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181ccd1dd8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=128, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[11602  1538]\n",
      " [  833  2308]]\n",
      "Confusion matrix, percentage of data:\n",
      "[[71.26097906  9.44659419]\n",
      " [ 5.11639334 14.17603341]]\n",
      "Confusion matrix interpretation:\n",
      " [['true negative' 'false negative']\n",
      " ['false positive' 'true positive']]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    confusion_matrix = np.array([\n",
    "        [0, 0], \n",
    "        [0, 0]\n",
    "    ])\n",
    "    pred = model.predict(X_test)\n",
    "    for i in range(len(pred)):\n",
    "        prediction = pred[i]\n",
    "        if prediction[0]>prediction[1] :\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "\n",
    "        expected = y_test[i][0]\n",
    "\n",
    "        confusion_matrix[prediction][expected] += 1\n",
    "    \n",
    "    return confusion_matrix\n",
    "confusion_matrix = evaluate_model(model, X_test, y_test)\n",
    "confusion_matrix_interpretation = np.array([\n",
    "        [\"true negative\", \"false negative\"], \n",
    "        [\"false positive\", \"true positive\"]\n",
    "    ])\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "print(\"Confusion matrix, percentage of data:\")\n",
    "print(confusion_matrix*100/sum(confusion_matrix.flatten()))\n",
    "print(\"Confusion matrix interpretation:\\n\", confusion_matrix_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[11602  1538]\n",
      " [  833  2308]]\n",
      "Confusion matrix, percentage of data:\n",
      "[[71.26097906  9.44659419]\n",
      " [ 5.11639334 14.17603341]]\n",
      "Confusion matrix interpretation:\n",
      " [['true negative' 'false negative']\n",
      " ['false positive' 'true positive']]\n",
      "Accuracy\n",
      "0.8543701246852159\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "print(\"Confusion matrix, percentage of data:\")\n",
    "print(confusion_matrix*100/sum(confusion_matrix.flatten()))\n",
    "print(\"Confusion matrix interpretation:\\n\", confusion_matrix_interpretation)\n",
    "print(\"Accuracy\")\n",
    "print((confusion_matrix[0][0] + confusion_matrix[1][1] + 0.0) / sum(confusion_matrix.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
